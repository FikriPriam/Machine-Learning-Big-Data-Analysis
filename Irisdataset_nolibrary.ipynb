{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Irisdataset_nolibrary.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FikriPriam/Machine-Learning-Big-Data-Analysis/blob/master/Irisdataset_nolibrary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "oosgztIBkdRS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "#Set working directory and load data\n",
        "os.chdir('D:\\Data')\n",
        "iris = pd.read_csv('iris.csv')\n",
        "#Create numeric classes for species (0,1,2) \n",
        "iris.loc[iris['Name']=='virginica','species']=0\n",
        "iris.loc[iris['Name']=='versicolor','species']=1\n",
        "iris.loc[iris['Name']=='setosa','species'] = 2\n",
        "iris = iris[iris['species']!=2]\n",
        "#Create Input and Output columns\n",
        "X = iris[['PetalLength', 'PetalWidth']].values.T\n",
        "Y = iris[['species']].values.T\n",
        "Y = Y.astype('uint8')\n",
        "#Make a scatter plot\n",
        "plt.scatter(X[0, :], X[1, :], c=Y[0,:], s=40, cmap=plt.cm.Spectral);\n",
        "plt.title(\"IRIS DATA | Blue - Versicolor, Red - Virginica \")\n",
        "plt.xlabel('Petal Length')\n",
        "plt.ylabel('Petal Width')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "24DJrRaAkhG4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \n",
        "    np.random.seed(2) # we set up a seed so that our output matches ours although the initialization is random.\n",
        "    \n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01 #weight matrix of shape (n_h, n_x)\n",
        "    b1 = np.zeros(shape=(n_h, 1))  #bias vector of shape (n_h, 1)\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01   #weight matrix of shape (n_y, n_h)\n",
        "    b2 = np.zeros(shape=(n_y, 1))  #bias vector of shape (n_y, 1)\n",
        "       \n",
        "    #store parameters into a dictionary    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters\n",
        "#Function to define the size of the layer\n",
        "def layer_sizes(X, Y):\n",
        "    n_x = X.shape[0] # size of input layer\n",
        "    n_h = 6# size of hidden layer\n",
        "    n_y = Y.shape[0] # size of output layer\n",
        "    return (n_x, n_h, n_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNgBPpBwkp17",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "#retrieve intialized parameters from dictionary    \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probability)\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.tanh(Z1)  #tanh activation function\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = 1/(1+np.exp(-Z2))  #sigmoid activation function\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WtMw29rxk101",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(A2, Y, parameters):\n",
        "   \n",
        "    m = Y.shape[1] # number of training examples\n",
        "    \n",
        "    # Retrieve W1 and W2 from parameters\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    \n",
        "    # Compute the cross-entropy cost\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
        "    cost = - np.sum(logprobs) / m\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bbuOEuJgk5X6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "# Number of training examples\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "    # Retrieve A1 and A2 from dictionary \"cache\".\n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    dZ2= A2 - Y\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hup06B-9k8ls",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate=0.8):\n",
        "# Retrieve each parameter from the dictionary \"parameters\"\n",
        "W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}